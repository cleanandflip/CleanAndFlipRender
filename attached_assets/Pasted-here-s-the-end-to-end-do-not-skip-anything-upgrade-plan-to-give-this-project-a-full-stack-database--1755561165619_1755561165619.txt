here’s the end-to-end, do-not-skip-anything upgrade plan to give this project a full-stack database feature with:

clean Dev/Prod branch separation

one-click checkpoints & rollbacks (like Replit’s code checkpoints)

crystal-clear change history

an /admin UI that shows tables/columns/indices for each branch

safe, auditable operations

I’m writing this as direct implementation instructions. Build only on top of the existing /admin area.

0) Goals & principles

Two branches, two URLs: DEV_DATABASE_URL and PROD_DATABASE_URL. Never reuse a single URL across environments.

Migrations are the source of truth (no ad-hoc SQL in app code).

Checkpoints = database snapshots you can create, list, diff, restore.

Rollbacks = either migrate “down” N steps or restore from a checkpoint snapshot.

Admin UX must be read-only by default; destructive operations use a typed confirm and require admin role.

Every action is logged (who, when, what, why).

1) Packages & project wiring

Install (server):

pnpm add pg @neondatabase/serverless zod
pnpm add -D node-pg-migrate tsx


We’ll use node-pg-migrate for deterministic, reversible migrations. You can keep your current ORM / query layer unchanged.

Directory layout (server)
server/
  db/
    registry.ts          # connection pools for dev/prod
    admin.sql.ts         # introspection queries
    migrations/          # node-pg-migrate files (timestamped)
  routes/
    admin-db.ts          # REST API for admin DB features
  middleware/
    require-admin.ts     # gate admin endpoints

2) Environment & secrets

Add to .env (or your Replit Secrets):

DEV_DATABASE_URL=postgres://<dev-credentials>
PROD_DATABASE_URL=postgres://<prod-credentials>
DB_SSL=true


Keep both URLs set. We’ll never infer one from the other.

3) DB connection registry

server/db/registry.ts

import { Pool } from "pg";

const ssl =
  process.env.DB_SSL === "true"
    ? { rejectUnauthorized: false }
    : undefined;

type Branch = "dev" | "prod";

const pools: Record<Branch, Pool> = {
  dev: new Pool({ connectionString: process.env.DEV_DATABASE_URL!, ssl }),
  prod: new Pool({ connectionString: process.env.PROD_DATABASE_URL!, ssl }),
};

export function getPool(branch: Branch) {
  return pools[branch];
}

4) Introspection helpers (tables, columns, indexes, row counts)

server/db/admin.sql.ts

import { getPool } from "./registry";

export async function listTables(branch: "dev" | "prod") {
  const db = getPool(branch);
  const q = `
    select
      t.table_schema,
      t.table_name,
      t.table_type,
      pg_total_relation_size(format('%I.%I', t.table_schema, t.table_name)) as total_bytes
    from information_schema.tables t
    where t.table_schema not in ('pg_catalog','information_schema')
    order by t.table_schema, t.table_name;
  `;
  const { rows } = await db.query(q);
  return rows;
}

export async function tableColumns(branch: "dev" | "prod", schema: string, table: string) {
  const db = getPool(branch);
  const q = `
    select
      column_name,
      data_type,
      is_nullable,
      column_default,
      ordinal_position
    from information_schema.columns
    where table_schema = $1 and table_name = $2
    order by ordinal_position;
  `;
  const { rows } = await db.query(q, [schema, table]);
  return rows;
}

export async function tableIndexes(branch: "dev" | "prod", schema: string, table: string) {
  const db = getPool(branch);
  const q = `
    select
      i.relname as index_name,
      pg_get_indexdef(ix.indexrelid) as definition,
      ix.indisunique as is_unique,
      ix.indisprimary as is_primary
    from pg_class t
    join pg_namespace n on n.oid = t.relnamespace
    join pg_index ix on t.oid = ix.indrelid
    join pg_class i on i.oid = ix.indexrelid
    where n.nspname = $1 and t.relname = $2
    order by is_primary desc, is_unique desc, index_name;
  `;
  const { rows } = await db.query(q, [schema, table]);
  return rows;
}

export async function migrationHistory(branch: "dev" | "prod") {
  const db = getPool(branch);
  // node-pg-migrate default table
  const q = `select id, name, run_on from pgmigrations order by run_on desc;`;
  try {
    const { rows } = await db.query(q);
    return rows;
  } catch {
    return []; // table not created yet
  }
}

5) Migrations (reversible, listable, auditable)
Configure node-pg-migrate

Add to package.json:

{
  "scripts": {
    "db:create": "node-pg-migrate create -m server/db/migrations",
    "db:up:dev":  "DATABASE_URL=$DEV_DATABASE_URL node-pg-migrate up -m server/db/migrations",
    "db:down:dev":"DATABASE_URL=$DEV_DATABASE_URL node-pg-migrate down -m server/db/migrations",
    "db:up:prod": "DATABASE_URL=$PROD_DATABASE_URL node-pg-migrate up -m server/db/migrations",
    "db:down:prod":"DATABASE_URL=$PROD_DATABASE_URL node-pg-migrate down -m server/db/migrations",
    "db:status:dev":  "DATABASE_URL=$DEV_DATABASE_URL node-pg-migrate status -m server/db/migrations",
    "db:status:prod": "DATABASE_URL=$PROD_DATABASE_URL node-pg-migrate status -m server/db/migrations"
  }
}


Create a migration: pnpm db:create add_users_table then implement up and down.
Deploy to dev: pnpm db:up:dev
Promote to prod: pnpm db:up:prod
Rollback 1 step prod: pnpm db:down:prod

Example migration server/db/migrations/1710000000000_add_users.ts

import { MigrationBuilder } from "node-pg-migrate";

export async function up(pgm: MigrationBuilder) {
  pgm.createTable("users", {
    id: "id",
    email: { type: "text", notNull: true, unique: true },
    created_at: { type: "timestamptz", default: pgm.func("now()") },
  });
  pgm.createIndex("users", "email");
}

export async function down(pgm: MigrationBuilder) {
  pgm.dropTable("users");
}


Why this choice: deterministic, reversible, single table pgmigrations tracks the exact applied list—perfect for our “list changes” UI.

6) Checkpoints & Rollbacks (snapshot/restore)

You get two safe options. Enable both:

A) Migration rollbacks (quick)

Use pnpm db:down:<branch> to revert 1 or N steps (repeat).

Best for immediate hotfixes when the last migration(s) caused trouble.

B) Database checkpoints (snapshot → restore)

Create a “checkpoint” before any potentially destructive change.

Implementation: use your database provider’s branching/snapshot feature (e.g., Neon branches or PITR) or run a full logical backup.

Minimal vendor-agnostic implementation

Add scripts (server-side tools) to:

Create checkpoint (namespaced): checkpoint:create <branch> "<label>"

Store metadata in admin_checkpoints table in that branch:

id, label, created_by, created_at, strategy (branch | dump), reference (e.g., provider branch name or dump file key), notes.

List checkpoints: for Dev or Prod.

Restore checkpoint (dangerous): requires typed phrase "RESTORE <checkpoint-id>".

Strategy branch: switch the branch’s connection string to the saved provider branch (or recreate from the saved time/LSN).

Strategy dump: run a full restore from the dumped archive.

If you are on Neon, implement checkpoint “create” by creating a new branch from the current one and saving the branch name into admin_checkpoints.reference. Restore by pointing PROD_DATABASE_URL to that branch (or moving data by dump/restore).
If you’re not on a provider with branches, use pg_dump/pg_restore to S3 (or similar).

Audit schema

Create once via a migration:

import { MigrationBuilder } from "node-pg-migrate";

export async function up(pgm: MigrationBuilder) {
  pgm.createTable("admin_checkpoints", {
    id: "id",
    label: { type: "text", notNull: true },
    created_by: { type: "text", notNull: true },
    created_at: { type: "timestamptz", default: pgm.func("now()") },
    branch: { type: "text", notNull: true, check: "branch in ('dev','prod')" },
    strategy: { type: "text", notNull: true }, // 'branch' | 'dump'
    reference: { type: "text", notNull: true }, // branch name or dump key
    notes: { type: "text" },
  });

  pgm.createTable("admin_actions", {
    id: "id",
    actor_id: { type: "text" },
    action: { type: "text", notNull: true },
    details: { type: "jsonb", notNull: true, default: "{}" },
    created_at: { type: "timestamptz", default: pgm.func("now()") },
  });
}

export async function down(pgm: MigrationBuilder) {
  pgm.dropTable("admin_actions");
  pgm.dropTable("admin_checkpoints");
}


Every migration run / checkpoint create / restore posts a row to admin_actions.

7) Admin API (server)

Gate everything behind admin. Use your existing auth roles.

server/middleware/require-admin.ts

import { Request, Response, NextFunction } from "express";
export function requireAdmin(req: any, res: Response, next: NextFunction) {
  const isAdmin = req.user?.role === "admin" || req.session?.role === "admin";
  if (!isAdmin) return res.status(403).json({ error: "Admin only" });
  next();
}


server/routes/admin-db.ts

import { Router } from "express";
import { z } from "zod";
import { requireAdmin } from "../middleware/require-admin";
import { listTables, tableColumns, tableIndexes, migrationHistory } from "../db/admin.sql";
import { getPool } from "../db/registry";
import { spawn } from "node:child_process";

const r = Router();
r.use(requireAdmin);

const Branch = z.enum(["dev","prod"]);

r.get("/api/admin/db/branches", (_req, res) =>
  res.json([{ key: "dev", url: process.env.DEV_DATABASE_URL ? "configured" : "missing" },
            { key: "prod", url: process.env.PROD_DATABASE_URL ? "configured" : "missing" }])
);

r.get("/api/admin/db/:branch/tables", async (req, res) => {
  const branch = Branch.parse(req.params.branch);
  res.json(await listTables(branch));
});

r.get("/api/admin/db/:branch/tables/:schema/:table", async (req, res) => {
  const branch = Branch.parse(req.params.branch);
  const { schema, table } = req.params;
  const [columns, indexes] = await Promise.all([
    tableColumns(branch, schema, table),
    tableIndexes(branch, schema, table),
  ]);
  res.json({ columns, indexes });
});

r.get("/api/admin/db/:branch/migrations", async (req, res) => {
  const branch = Branch.parse(req.params.branch);
  res.json(await migrationHistory(branch));
});

// Run migrations UP or DOWN (typed confirmation for prod)
r.post("/api/admin/db/:branch/migrate", async (req: any, res) => {
  const branch = Branch.parse(req.params.branch);
  const direction = z.enum(["up","down"]).parse(req.query.dir ?? "up");
  const steps = Number(req.query.steps ?? 1);

  if (branch === "prod" && direction === "down") {
    if (req.body?.confirm !== `ROLLBACK ${steps}`) {
      return res.status(400).json({ error: "Type confirm phrase: ROLLBACK <steps>" });
    }
  }

  const env = { ...process.env, DATABASE_URL: branch === "dev" ? process.env.DEV_DATABASE_URL! : process.env.PROD_DATABASE_URL! };
  const args = [direction, "-m", "server/db/migrations"];
  const child = spawn("node-pg-migrate", args, { env });

  let out = ""; let err = "";
  child.stdout.on("data", (d) => (out += d.toString()));
  child.stderr.on("data", (d) => (err += d.toString()));

  child.on("close", (code) => {
    res.status(code === 0 ? 200 : 500).json({ ok: code === 0, out, err });
  });
});

export default r;


Mount it in server/index.ts after auth middleware:

import adminDBRoutes from "./routes/admin-db";
app.use(adminDBRoutes);

8) Admin UI (client) – new Database tab (keep UI consistent)

Add a new tab to your existing /admin page:

Branch selector: Development | Production

Tables list (schema, table, type, size)

Table details: columns (name, type, nullable, default), indexes, PK/unique

Migrations: applied list (name, run_on), “Status: up-to-date / N pending”

Actions group (admin-only, all behind confirmations):

Create checkpoint (label + optional notes)

Run migration up (Dev only)

Promote Dev → Prod (explain: run the same migrations on Prod)

Rollback last N (Prod requires ROLLBACK N)

Restore from checkpoint (Prod requires RESTORE <checkpoint-id>)

API usage

GET /api/admin/db/branches

GET /api/admin/db/:branch/tables

GET /api/admin/db/:branch/tables/:schema/:table

GET /api/admin/db/:branch/migrations

POST /api/admin/db/:branch/migrate?dir=up|down&steps=1 (JSON body for confirm)

Keep Dev operations easy; Prod operations always require an explicit confirmation phrase and double prompt.

9) Promotion flow (Dev → Prod)

Apply / test migrations on Dev
pnpm db:up:dev

Create a Prod checkpoint with label (e.g., pre-2025-08-19-release-3)

Apply the same migrations on Prod
pnpm db:up:prod

If anything breaks, rollback:

Quick: pnpm db:down:prod (one step)

Full: Restore checkpoint (switch branch/restore dump)

All actions are recorded in admin_actions.

10) Observability, safety, and guardrails

Admin audit: every API route writes an admin_actions row (action, actor_id, details).

Resource limits: for large tables, row counts use pg_class.reltuples estimates instead of COUNT(*).

Read-only by default: no DELETE/UPDATE raw actions exposed in UI. Schema changes only through migrations.

DB timeouts: set statement_timeout low for admin endpoints:

set local statement_timeout = '15s';


Connection pooling: reuse the pools in registry.ts (do not create ad-hoc clients per request).

Backups: keep at least 7 days of checkpoint retention (if using provider branches/PITR).

11) CI/CD hooks (optional but recommended)

Pre-merge (Dev): run pnpm db:status:dev and pnpm test

Release (Prod): pnpm db:status:prod (should show pending), then pnpm db:up:prod

Auto-checkpoint: create a prod checkpoint before running db:up:prod.

12) Cleanup / purge legacy DB code

Remove or migrate anything that can silently mutate schema outside migrations. Specifically:

ad-hoc CREATE/ALTER/DROP statements in request handlers

any server/db/migrate*.ts one-offs or custom SQL folders not under server/db/migrations

duplicate environment variables like DATABASE_URL shadowing the two explicit ones

old admin routes that list tables using raw strings scattered across files—replace with admin.sql.ts helpers

Keep all schema-affecting SQL only inside node-pg-migrate migration files.

13) What the /admin Database tab must show (per branch)

Branch header: Development / Production (URL configured / missing)

Summary cards:

tables count, total DB size, last migration name/time, last checkpoint

Tables grid:

schema, table, type (BASE/VIEW), size, (optional) estimated row count

Table details drawer:

columns (name, type, nullability, default), indexes (unique/primary + definition), FK references (if you want to extend)

Migrations:

list of pgmigrations rows, newest first

“Pending” count (compare migrations on disk vs rows in table)

Checkpoints list:

label, created_by, created_at, strategy, reference

“Restore” button (Prod) with confirmation

Actions (role=admin only):

Create checkpoint (label + notes)

Run migration up (Dev)

Promote Dev → Prod (runs up on Prod)

Rollback last N (Prod only, typed confirm)

Restore from checkpoint (Prod only, typed confirm)

14) Developer workflows (simple & precise)
Make a schema change

pnpm db:create add_customer_status

Edit the generated migration up/down.

pnpm db:up:dev → test → seed/dev data if needed.

Create Prod checkpoint.

pnpm db:up:prod.

If needed: pnpm db:down:prod or restore checkpoint.

Check what changed

pnpm db:status:dev / pnpm db:status:prod (CLI)

/admin → Database → Migrations (Dev/Prod) (UI)

Rollback

1 step: pnpm db:down:prod

Many steps: run db:down:prod N times or use “Restore checkpoint”.

15) Edge cases we handle

Column missing errors (like earlier email_verified_at) → introduce migration to add it (and down to drop).

Hotfix on prod → create quick migration, db:up:prod, checkpoint first.

Auth → Admin routes require admin role; rate-limit POST actions.

Cache → After migrations that alter lookup tables, invalidate Redis (when you enable it) or app cache.

16) What you’ll deliver after this upgrade

server/db/registry.ts

server/db/admin.sql.ts

server/db/migrations/* (timestamped, reversible)

server/routes/admin-db.ts + require-admin.ts

Updated server/index.ts to mount admin DB routes

New Database tab in /admin with the UI described

admin_checkpoints + admin_actions tables

package.json scripts for dev/prod up/down/status

Documentation in README.md (summarize the flows and guardrails)