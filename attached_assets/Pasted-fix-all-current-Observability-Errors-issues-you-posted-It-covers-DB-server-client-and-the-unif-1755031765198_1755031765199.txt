fix all current Observability (Errors) issues you posted. It covers DB, server, client, and the unified dropdowns.

0) Route + nav: point to the new page and use our dropdowns
Rename the tab to “Errors” but route it to /admin/observability.

In src/config/routes.ts:

path: "/admin/errors" → redirect to "/admin/observability".

In src/pages/admin/observability-*.tsx:

Remove any @radix-ui/react-select, native <select>, or ad-hoc dropdowns.

Use our unified components/ui/Dropdown.tsx everywhere in the filter bar (Level, Environment, Status, Show ignored/Test events).

Quick grep to remove stragglers:

bash
Copy
Edit
rg -n "radix|@radix-ui/react-select|<select" src/pages/admin/observability
1) Database: create the missing hour column and required tables
Your logs show column "hour" does not exist. Create/align the schema for Postgres (Neon).

SQL migration (run once):

sql
Copy
Edit
-- errors_raw: individual captured events
CREATE TABLE IF NOT EXISTS errors_raw (
  id            BIGSERIAL PRIMARY KEY,
  event_id      TEXT NOT NULL,
  created_at    TIMESTAMPTZ NOT NULL DEFAULT now(),
  service       TEXT NOT NULL,                   -- 'client' | 'server'
  level         TEXT NOT NULL,                   -- 'error' | 'warn' | 'info'
  env           TEXT NOT NULL,                   -- 'production' | 'development'
  release       TEXT,
  url           TEXT,
  method        TEXT,
  status_code   INT,
  message       TEXT,
  type          TEXT,
  stack         TEXT,                            -- normalized multi-line string
  user_id       TEXT,
  tags          JSONB,
  extra         JSONB,
  fingerprint   TEXT NOT NULL
);

CREATE INDEX IF NOT EXISTS idx_errors_raw_fp      ON errors_raw(fingerprint);
CREATE INDEX IF NOT EXISTS idx_errors_raw_created ON errors_raw(created_at DESC);

-- issues: aggregated rollup per fingerprint
CREATE TABLE IF NOT EXISTS issues (
  fingerprint   TEXT PRIMARY KEY,
  title         TEXT NOT NULL,
  level         TEXT NOT NULL,
  first_seen    TIMESTAMPTZ NOT NULL,
  last_seen     TIMESTAMPTZ NOT NULL,
  count         BIGINT NOT NULL DEFAULT 0,
  affected_users BIGINT NOT NULL DEFAULT 0,
  resolved      BOOLEAN NOT NULL DEFAULT FALSE,
  ignored       BOOLEAN NOT NULL DEFAULT FALSE,
  sample_event_id TEXT,
  envs          JSONB NOT NULL DEFAULT '{}'
);

CREATE INDEX IF NOT EXISTS idx_issues_last_seen   ON issues(last_seen DESC);
CREATE INDEX IF NOT EXISTS idx_issues_res_ign     ON issues(resolved, ignored);

-- issue_events: hourly timeseries rollup
CREATE TABLE IF NOT EXISTS issue_events (
  fingerprint   TEXT NOT NULL,
  hour          TIMESTAMPTZ NOT NULL,   -- <<< this is the missing column
  count         BIGINT NOT NULL DEFAULT 0,
  PRIMARY KEY (fingerprint, hour)
);

CREATE INDEX IF NOT EXISTS idx_issue_events_hour ON issue_events(hour);
2) Server: fix 400s on /api/observability/errors and “Invalid time value”
The 400s mean our Zod schema is stricter than the client payload. Also “Invalid time value” occurs when hour (or dates) are null/undefined.

Ingest route (server/routes/observability-new.ts or your route file):

Relax the schema; accept missing fields gracefully.

Normalize dates and guard toISOString().

ts
Copy
Edit
import { z } from "zod";
import { ErrorStore } from "../data/errorStore"; // Postgres version
import { v4 as uuid } from "uuid";

const Ingest = z.object({
  service: z.enum(["client","server"]).default("client"),
  level: z.enum(["error","warn","info"]).default("error"),
  env: z.enum(["production","development"]).default(process.env.NODE_ENV==="production"?"production":"development"),
  release: z.string().optional(),
  url: z.string().optional(),
  method: z.string().optional(),
  statusCode: z.coerce.number().optional(),
  message: z.string().optional(),        // <- was required; now optional
  type: z.string().optional(),
  stack: z.string().optional(),
  user: z.object({ id: z.string().optional() }).optional(),
  tags: z.record(z.string(), z.any()).optional(),
  extra: z.record(z.string(), z.any()).optional(),
}).strip(); // ignore unknown keys

function normalizeStack(raw?: string): string {
  if (!raw) return "";
  return raw
    .split("\n")
    .map(l => l.trim())
    .filter(l => l && !/node_modules|\(internal/.test(l))
    .map(l => l.replace(/:\d+:\d+/g, ":__:__"))
    .join("\n");
}

function safeTitle(msg?: string, type?: string) {
  const base = (msg && msg.split("\n")[0]) || type || "Error";
  return base.slice(0, 160);
}

function fingerprintOf(p: {service:string; type?:string; message?:string; stack?:string; url?:string}) {
  const top = p.stack?.split("\n")[0] ?? "";
  const basis = [p.service, p.type ?? "", (p.message ?? "").slice(0,180), top, p.url ?? ""].join("|");
  let h = 0; for (let i=0;i<basis.length;i++) h = (h*31 + basis.charCodeAt(i))|0;
  return `fp_${Math.abs(h)}`;
}

app.post("/api/observability/errors", async (req, res) => {
  const parsed = Ingest.safeParse(req.body);
  if (!parsed.success) return res.status(200).json({ ok: true, ignored: true }); // don't spam 400s

  let d = parsed.data;
  // noise control
  const isBeacon = d.url?.endsWith("/public/js/beacon.js");
  const isCategoryImg = /res\.cloudinary\.com\/clean-flip\/image\/upload\/.*\/categories\//i.test(d.message ?? "") ||
                        /res\.cloudinary\.com\/clean-flip\/image\/upload\/.*\/categories\//i.test(d.url ?? "");
  if (isBeacon) return res.status(204).end();
  if (isCategoryImg && d.level === "error") d.level = "info";

  const now = new Date();
  const stack = normalizeStack(d.stack);
  const fingerprint = fingerprintOf({ service: d.service, type: d.type, message: d.message, stack, url: d.url });

  const raw = {
    event_id: uuid(),
    created_at: now,
    service: d.service,
    level: d.level,
    env: d.env,
    release: d.release,
    url: d.url,
    method: d.method,
    status_code: d.statusCode,
    message: d.message ?? d.type ?? "Error",
    type: d.type,
    stack,
    user_id: d.user?.id,
    tags: d.tags ?? {},
    extra: d.extra ?? {},
    fingerprint
  };

  await ErrorStore.insertRaw(raw);
  const issue = await ErrorStore.upsertIssue(raw);  // returns updated row
  await ErrorStore.bumpRollup(raw);                 // writes (fingerprint, hour, +1)

  return res.status(201).json({ ok: true, fingerprint, eventId: raw.event_id });
});
Series route: guard hour and dates.

ts
Copy
Edit
app.get("/api/observability/series", async (req, res) => {
  const days = Math.min(Number(req.query.days ?? 1), 30);
  const now = new Date();
  const from = new Date(now.getTime() - days*24*3600*1000);
  const rows = await ErrorStore.chartByHour({ from, to: now }); // must return hour TIMESTAMPTZ

  const data = rows
    .filter(r => r.hour) // guard
    .map(r => ({ hour: new Date(r.hour as any).toISOString(), count: Number(r.count) || 0 }));

  res.json(data);
});
3) ErrorStore (Postgres) must match the schema above
Ensure these write/read functions align with your SQL.

ts
Copy
Edit
// server/data/errorStore.ts  (Postgres w/ @neondatabase/serverless or pg)
export const ErrorStore = {
  async insertRaw(r: any) {
    await sql/*sql*/`
      INSERT INTO errors_raw
        (event_id, created_at, service, level, env, release, url, method, status_code,
         message, type, stack, user_id, tags, extra, fingerprint)
      VALUES
        (${r.event_id}, ${r.created_at}, ${r.service}, ${r.level}, ${r.env}, ${r.release},
         ${r.url}, ${r.method}, ${r.status_code}, ${r.message}, ${r.type}, ${r.stack},
         ${r.user_id}, ${sql.json(r.tags)}, ${sql.json(r.extra)}, ${r.fingerprint})
    `;
  },

  async upsertIssue(r: any) {
    const title = (r.message?.split("\n")[0] ?? r.type ?? "Error").slice(0, 160);
    const envsInc = sql`COALESCE(envs, '{}'::jsonb) || jsonb_build_object(${r.env}, COALESCE((envs->>${r.env})::int, 0) + 1)`;

    const row = await sql/*sql*/`
      INSERT INTO issues (fingerprint, title, level, first_seen, last_seen, count, resolved, ignored, sample_event_id, envs)
      VALUES (${r.fingerprint}, ${title}, ${r.level}, ${r.created_at}, ${r.created_at}, 1, FALSE, FALSE, ${r.event_id}, jsonb_build_object(${r.env}, 1))
      ON CONFLICT (fingerprint) DO UPDATE
        SET last_seen = EXCLUDED.last_seen,
            count     = issues.count + 1,
            envs      = ${envsInc}
      RETURNING *;
    `;
    return row[0];
  },

  async bumpRollup(r: any) {
    // bucket to hour
    const hour = new Date(r.created_at); hour.setMinutes(0,0,0);
    await sql/*sql*/`
      INSERT INTO issue_events (fingerprint, hour, count)
      VALUES (${r.fingerprint}, ${hour.toISOString()}, 1)
      ON CONFLICT (fingerprint, hour) DO UPDATE
        SET count = issue_events.count + 1
    `;
  },

  async chartByHour({ from, to }: {from: Date; to: Date}) {
    const rows = await sql/*sql*/`
      SELECT hour, SUM(count)::bigint AS count
      FROM issue_events
      WHERE hour BETWEEN ${from.toISOString()} AND ${to.toISOString()}
      GROUP BY hour
      ORDER BY hour ASC
    `;
    return rows;
  },
};
4) Client page: wire filters + unified dropdowns + loading
In src/pages/admin/observability-complete.tsx (or your file):

Use Dropdown (components/ui/Dropdown.tsx) for Level / Environment / Status / Show ignored / Show test events.

Query: GET /api/observability/issues?q=&level=error&resolved=false&ignored=false&page=1&limit=20

On first load, if nothing exists, show “No issues yet” instead of “Loading…” forever.

If the request fails, surface a toast and render empty state.

Minimal fetch (React Query is fine; below is plain fetch shape):

ts
Copy
Edit
const [filters, setFilters] = useState({
  q: "", level: "error", env: "", resolved: false, ignored: false, page: 1, limit: 20
});

useEffect(() => {
  let cancelled = false;
  setLoading(true);
  const params = new URLSearchParams(Object.entries(filters).map(([k,v]) => [k, String(v)]));

  fetch(`/api/observability/issues?${params}`, { credentials: "include" })
    .then(r => r.ok ? r.json() : Promise.reject(r))
    .then(data => { if (!cancelled) { setData(data); setLoading(false); } })
    .catch(() => { if (!cancelled) { setError(true); setLoading(false); } });

  return () => { cancelled = true; };
}, [filters]);
Empty state logic:

tsx
Copy
Edit
if (loading) return <Card>Loading…</Card>;
if (error) return <Card>Couldn’t load issues. Try again.</Card>;
if (!data || data.total === 0) return <Card>No issues found for current filters.</Card>;
5) Sanitizer allowlist & middleware order
Ensure express.json() runs before the sanitizer and routes.

Allow:

/api/observability/errors

/api/observability/issues*

/api/observability/series*

6) Quick verifications to run
bash
Copy
Edit
# DB check
psql "$DATABASE_URL" -c "\d issue_events"      # must show column 'hour' timestamptz

# API smoke
curl -s -X POST http://localhost:5000/api/observability/errors \
  -H "content-type: application/json" \
  -d '{"service":"client","level":"error","env":"development","message":"Smoke test","url":"/admin/observability"}'

curl -s "http://localhost:5000/api/observability/issues?level=error&resolved=false&ignored=false&page=1&limit=20"

curl -s "http://localhost:5000/api/observability/series?days=1"
You should see:

POST /observability/errors → 201 (no more 400s)

Issues list → JSON with items (or empty with total: 0, but no “Loading…” hang)

Series → array of { hour, count } ISO-strings (no “Invalid time value”)

7) Performance notes you saw in logs
Those “Slow request detected” messages during dev are Vite/HMR + large CSS; they’re expected in dev. No action needed.

Once you apply the above:

The issues load reliably.

The filter bar uses the unified dropdown component.

The ingest endpoint accepts your current client payload (no more 400s).

The chart works (table + column fixed).

No toISOString crash.








Ask ChatGPT
